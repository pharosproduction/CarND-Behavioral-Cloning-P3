{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.models import model_from_json\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers.advanced_activations import ELU\n",
    "from keras.layers import Flatten, Dense, Lambda, Dropout\n",
    "from keras.layers.convolutional import Convolution2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_csv(directory):\n",
    "    with open(directory + '/driving_log.csv', 'r') as csvfile:\n",
    "        lines = []\n",
    "        reader = csv.reader(csvfile)\n",
    "        \n",
    "        for i, line in enumerate(reader):\n",
    "            if i != 0:\n",
    "                lines.append(line)\n",
    "    \n",
    "        return lines\n",
    "    \n",
    "def load_image(source_path):\n",
    "    file_dir = source_path.split('/')[-4:-1]\n",
    "    filename = source_path.split('/')[-1]\n",
    "    path = os.path.join(*file_dir) + '/' + filename\n",
    "    image = cv2.imread(path)\n",
    "    image = image[60:140, :, :] # y1:y2, x1:x2\n",
    "    image = cv2.GaussianBlur(image, (3,3), 0)\n",
    "    image = cv2.resize(image,(200, 66), interpolation = cv2.INTER_AREA)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2YUV)\n",
    "    \n",
    "    return image.astype(np.uint8)\n",
    "\n",
    "def build_generator(files, steerings, batch_size):\n",
    "    while True:\n",
    "        X = np.zeros((0, 66, 200, 3), dtype=np.uint8)\n",
    "        y = np.zeros((0, 1), dtype=np.float16)\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            idx = np.random.choice(len(files), 1)[0]\n",
    "            feature = load_image(files[idx])\n",
    "            X = np.append(X, feature[None,:], axis=0)\n",
    "            flipped_feature = cv2.flip(feature, 1)\n",
    "            X = np.append(X, flipped_feature[None,:], axis=0)\n",
    "            \n",
    "            label = steerings[idx]\n",
    "            y = np.append(y, label)\n",
    "            y = np.append(y, -1 * label)\n",
    "        \n",
    "        yield shuffle(X, y)\n",
    "        \n",
    "def remove_low_steering(files, steerings):\n",
    "    filtered_files = []\n",
    "    filtered_steerings = []\n",
    "    \n",
    "    for idx, steering in enumerate(steerings):\n",
    "        if abs(steering) >= 0.15:\n",
    "            filtered_files.append(files[idx])\n",
    "            filtered_steerings.append(steering)\n",
    "        else:\n",
    "            random = np.random.randint(10)\n",
    "            \n",
    "            if random < 10:\n",
    "                filtered_files.append(files[idx])\n",
    "                filtered_steerings.append(steering)\n",
    "\n",
    "    print(\"Dropped {} rows with low steering\".format(len(files) - len(filtered_files)))\n",
    "    return filtered_files, filtered_steerings\n",
    "        \n",
    "def filter_bias_data(files, steerings):\n",
    "    files, steerings = remove_low_steering(files, steerings)\n",
    "    \n",
    "    num_bins = 100\n",
    "    avg_samples_per_bin = int(math.ceil(len(steerings) / num_bins))\n",
    "    hist, bins = np.histogram(steerings, num_bins)\n",
    "    width = 0.7 * (bins[1] - bins[0])\n",
    "    center = (bins[:-1] + bins[1:]) / 2\n",
    "    plt.bar(center, hist, align='center', width=width)\n",
    "    plt.plot((np.min(steerings), np.max(steerings)), (avg_samples_per_bin, avg_samples_per_bin), 'k-')\n",
    "\n",
    "    keep_probs = []\n",
    "    target = avg_samples_per_bin * 0.2\n",
    "    remove_list = []\n",
    "    \n",
    "    for i in range(num_bins):\n",
    "        if hist[i] < target:\n",
    "            keep_probs.append(1.0)\n",
    "        else:\n",
    "            keep_probs.append(1.0 / (hist[i] / target))\n",
    "\n",
    "    for i in range(len(steerings)):\n",
    "        for j in range(num_bins):\n",
    "            if steerings[i] > bins[j] and steerings[i] <= bins[j+1]:\n",
    "                if np.random.rand() > keep_probs[j]:\n",
    "                    remove_list.append(i)\n",
    "                    \n",
    "    files_filtered = np.delete(files, remove_list, axis=0)\n",
    "    steerings_filtered = np.delete(steerings, remove_list)\n",
    "\n",
    "    hist, bins = np.histogram(steerings_filtered, num_bins)\n",
    "    plt.bar(center, hist, align='center', width=width)\n",
    "    plt.plot((np.min(steerings_filtered), np.max(steerings_filtered)), (avg_samples_per_bin, avg_samples_per_bin), 'k-')\n",
    "\n",
    "    f, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    ax1.hist(steerings, bins=num_bins)\n",
    "    ax2.hist(steerings_filtered, bins=num_bins)\n",
    "    plt.show()\n",
    "    plt.close(f)\n",
    "    \n",
    "    return files_filtered, steerings_filtered\n",
    "    \n",
    "def load_data(directory):\n",
    "    csv_lines = read_csv(directory)\n",
    "#     f, ax_list = plt.subplots(1, 3, figsize=(10, 4))\n",
    "    \n",
    "#     for i in range(3):\n",
    "#         img = load_image(csv_lines[0][i])\n",
    "#         ax_list[i].imshow(img, cmap='gray')\n",
    "#         ax_list[i].axis(\"off\")\n",
    "\n",
    "#     plt.show()\n",
    "    \n",
    "    return csv_lines\n",
    "\n",
    "def split_on_batches(train_samples, valid_samples, size=32):\n",
    "    train_lines_num = len(train_samples)\n",
    "    train_batches = train_lines_num // size\n",
    "    train_samples_num = size * train_batches\n",
    "    train_csv_lines = train_samples[:train_samples_num]\n",
    "    train = train_csv_lines, train_batches\n",
    "    \n",
    "    valid_lines_num = len(valid_samples)\n",
    "    valid_batches = valid_lines_num // size\n",
    "    valid_samples_num = size * valid_batches\n",
    "    valid_csv_lines = valid_samples[:valid_samples_num]\n",
    "    valid = valid_csv_lines, valid_batches\n",
    "    \n",
    "    return train, valid, size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_model(model_path, train, valid, batch_size, epochs):\n",
    "    train_data, train_batches = train\n",
    "    train_files, train_steerings = zip(*train_data)\n",
    "    \n",
    "    valid_data, valid_batches = valid\n",
    "    valid_files, valid_steerings = zip(*valid_data)\n",
    "\n",
    "    train_samples_num = len(train_files)\n",
    "    train_generator = build_generator(train_files, train_steerings, batch_size)\n",
    "    valid_samples_num = len(valid_files)\n",
    "    valid_generator = build_generator(valid_files, valid_steerings, batch_size)\n",
    "\n",
    "    model = None\n",
    "    show_summary = False\n",
    "    \n",
    "    if Path(model_path + \".json\").is_file():\n",
    "        with open(model_path + \".json\", \"r\") as json_file:\n",
    "            loaded_model_json = json_file.read()\n",
    "            json_file.close()\n",
    "            model = model_from_json(loaded_model_json)\n",
    "            model.load_weights(model_path + \".h5\")\n",
    "            print(\"Loaded model from disk\")\n",
    "    else:\n",
    "        show_summary = True\n",
    "        \n",
    "    if model is None:\n",
    "        print(\"Create new model\")\n",
    "        model = Sequential()\n",
    "        model.add(Lambda(lambda x: x / 255.0 - 0.5, input_shape=(66,200,3)))\n",
    "\n",
    "        model.add(Convolution2D(24, 5, 5, subsample=(2, 2), border_mode='valid'))\n",
    "        model.add(ELU())\n",
    "        model.add(Convolution2D(36, 5, 5, subsample=(2, 2), border_mode='valid'))\n",
    "        model.add(ELU())\n",
    "        model.add(Convolution2D(48, 5, 5, subsample=(2, 2), border_mode='valid'))\n",
    "        model.add(ELU())\n",
    "        model.add(Convolution2D(64, 3, 3, border_mode='valid'))\n",
    "        model.add(ELU())\n",
    "        model.add(Convolution2D(64, 3, 3, border_mode='valid'))\n",
    "        model.add(ELU())\n",
    "        model.add(Dropout(0.4))\n",
    "        \n",
    "        model.add(Flatten())\n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(ELU())\n",
    "        \n",
    "        model.add(Dense(100))\n",
    "        model.add(ELU())\n",
    "\n",
    "        model.add(Dense(50))\n",
    "        model.add(ELU())\n",
    "\n",
    "        model.add(Dense(10))\n",
    "        model.add(ELU())\n",
    "\n",
    "        model.add(Dense(1))\n",
    "    \n",
    "    optimizer = Adam(lr=1e-4)\n",
    "    model.compile(loss='mse', optimizer=optimizer)\n",
    "\n",
    "    history = model.fit_generator(train_generator, samples_per_epoch=2*train_samples_num, validation_data=valid_generator, nb_val_samples=2*valid_samples_num, nb_epoch=epochs, verbose=1)\n",
    "    print(history.history)\n",
    "    \n",
    "    model.save_weights(model_path + \".h5\")\n",
    "    model_json = model.to_json()\n",
    "    with open(model_path + \".json\", \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "\n",
    "    if show_summary:\n",
    "        model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-0c882adfe612>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mdirectory\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdirectories\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data/\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'load_data' is not defined"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    files = []\n",
    "    steerings = []\n",
    "    # directories = [\"Track1.1\", \"Track1.2\", \"Track1.3\", \"Track1.4\", \"Track1.5\", \"Track1.6\", \n",
    "    #                \"Track1.7 - left border\",\"Track1.8 - borders\", \"Track1.9 - full\", \"Track1.9.1 - full\", \n",
    "    #                \"Track1.9.2 - full\", \"Track1.9.3 - full\", \"Track1.9.4 - full\", \"Track2\", \n",
    "    #                \"Track2.1 - full\", \"Track2.2 - full\"]\n",
    "\n",
    "#     directories = [\"Track1.9.2 - full\", \"Track2.1 - full\"]\n",
    "#     directories = [\"Track1.9.1 - full\", \"Track2\"]\n",
    "    directories = [\"Track1.3\", \"Track1.4\", \"Track1.5\", \"Track1.9.1 - full\", \"Track1.9.2 - full\", \"Track1.9.3 - full\", \"Track1.9.4 - full\"]\n",
    "#     directories = [\"Track1.3\", \"Track1.4\", \"Track1.5\"]\n",
    "    steering_offsets = [0.0, 0.1, -0.1] # center, left, right\n",
    "\n",
    "    for directory in directories:\n",
    "        lines = load_data(\"data/\" + directory)\n",
    "\n",
    "        for line in lines:\n",
    "            for idx, offset in enumerate(steering_offsets):\n",
    "                offset = float(line[3]) + steering_offsets[idx]\n",
    "                steerings.append(offset)\n",
    "                files.append(line[idx])\n",
    "\n",
    "    files, steerings = filter_bias_data(files, steerings)\n",
    "\n",
    "    train_samples, valid_samples = train_test_split(list(zip(files, steerings)), test_size=0.2)\n",
    "    train, valid, size = split_on_batches(train_samples, valid_samples)\n",
    "    train_model(\"model/p3_model\", train, valid, size, epochs=5)\n",
    "    \n",
    "print(\"We're done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
